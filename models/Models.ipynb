{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBl7pxTif57Wes1RNDVaTP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anil-matcha/Action_Recognition/blob/master/models/Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models\n",
        "\n",
        "In the previous lesson we have covered the basics of Langchain. Here is the link to the first lesson https://github.com/SamurAIGPT/langchain-course/blob/main/getting-started/Introduction.ipynb\n",
        "\n",
        "In this lesson we will talk about different kinds of models including LLMs, Chat Models and Embedding Models\n",
        "\n",
        "### What is a model ?\n",
        "\n",
        "A model is a program which is trained to complete a specific task. Since our course is about language tasks the models we will be using are language models. A model is trained on a huge corpus of data\n",
        "\n",
        "We are not going to cover the process of training as we are going to use already trained models. These pre-trained models are trained on large amounts of data and require a lot of compute to run and thus are called Large Language Models (LLM)"
      ],
      "metadata": {
        "id": "-dBjFwloRjWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. LLM (Large Language Model)\n",
        "\n",
        "Now let's talk about LLM. LLM are trained to do language tasks like text generation. There are various LLM in the market but we are going to cover only OpenAI LLM since they are the most popular. OpenAI has 4 such models Davinci, Curie, Ada and Babbage\n",
        "\n",
        "##### Here is how you can use OpenAI LLM in langchain\n",
        "\n",
        "##### Let's install necessary libraries\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "D39TvmO0XwVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxpekH8sY51F",
        "outputId": "5612d231-3814-4cff-a58e-45261c85237f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.180)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.7)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.7)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(temperature=1, openai_api_key=\"openai-key\")\n",
        "print(llm(\"Tell me a joke\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY2ySwxrR6FA",
        "outputId": "ef7623ac-3d45-4fde-9c52-505145cad3b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Q: What did the fish say when he hit the wall?\n",
            "A: Dam!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimating number of tokens\n",
        "\n",
        "OpenAI models have a context length limiting the size of input data which can be sent to the model. Thus we need to make sure the input text is below that limit before sending to the model. We can do that calculation using the code below"
      ],
      "metadata": {
        "id": "rE2dyU_7jFdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.get_num_tokens(\"what a joke\")"
      ],
      "metadata": {
        "id": "aJ8nKBryZJ05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8517fd34-08a1-4bd2-cb25-38fbe30b5bbf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming\n",
        "\n",
        "Streaming is a major concept in LLM which allows you to display output on the go instead of waiting for the full output. Even in the ChatGPT interface you will see content streamed instead of waiting till entire output is generated\n",
        "\n",
        "Here is a code example for the same. We handle streaming in langchain using a callback handler"
      ],
      "metadata": {
        "id": "39MZ8q1vkRmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0, openai_api_key=\"openai-key\")\n",
        "resp = llm(\"Write me a poem on beauty.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg_l58n4jelZ",
        "outputId": "ab2c4ab4-0ee6-4cb1-b171-8bef5e9a65ba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Beauty is a thing of wonder,\n",
            "A sight that can't be denied.\n",
            "It's a feeling that can't be measured,\n",
            "A feeling that can't be denied.\n",
            "\n",
            "It's a thing of beauty that can't be seen,\n",
            "But can be felt in the heart and soul.\n",
            "It's a thing of beauty that can't be touched,\n",
            "But can be seen in the eyes of the beholder.\n",
            "\n",
            "Beauty is a thing of joy,\n",
            "A thing that can't be taken away.\n",
            "It's a thing of beauty that can't be bought,\n",
            "But can be found in the simplest of things.\n",
            "\n",
            "Beauty is a thing of love,\n",
            "A thing that can't be denied.\n",
            "It's a thing of beauty that can't be measured,\n",
            "But can be seen in the love of others.\n",
            "\n",
            "Beauty is a thing of life,\n",
            "A thing that can't be taken away.\n",
            "It's a thing of beauty that can't be bought,\n",
            "But can be found in the simplest of things."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Chat Models\n",
        "\n",
        "The second set of models we are going to cover are the chat models. The famous ChatGPT model GPT-3.5 comes under this. The main difference between the previous LLM models and the Chat Models are\n",
        "\n",
        "* Chat Models are 10x cheaper for api calls\n",
        "* You can hold a conversation with a chat model like you can with a human which is not possible with the previous LLMs\n",
        "\n",
        "Since Chat Models can hold a conversation they take a list of chat messages as input instead of plain text like a LLM\n",
        "\n",
        "Now let's discuss how we can use these Chat Models. Let's do the necessary imports"
      ],
      "metadata": {
        "id": "a6POXNT5lISE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")"
      ],
      "metadata": {
        "id": "-vt0WitNkGbO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using a OpenAI class we will be using a ChatOpenAI class to create our chat LLM"
      ],
      "metadata": {
        "id": "PQCg5peknI4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(temperature=0, openai_api_key=\"openai-key\")"
      ],
      "metadata": {
        "id": "HvNAJfqJnETc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input is a bunch of messages. Messages are classified into 3 types\n",
        "\n",
        "* System Message - This is an initial prompt sent to the model to control the behavior of the model\n",
        "\n",
        "* Human Message  - Input message of the user\n",
        "\n",
        "* AI Message     - Message response given by ChatGPT\n",
        "\n",
        "ChatGPT needs a list of all these messages in the conversation to be able to understand the content and converse further\n",
        "\n",
        "Now let's see an example where we define the system message and the message input of the user and pass to the chat model. The output generated will be an AI message\n",
        "\n",
        "We are using a System Prompt to let the model do the task of paraphrasing. This technique of providing the model a prompt to make it perform a task is called Prompt Engineering and can be part of another lesson"
      ],
      "metadata": {
        "id": "tJwiLHVWnbvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that paraphrases the sentence.\"),\n",
        "    HumanMessage(content=\"I love programming.\")\n",
        "]\n",
        "chat(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhyJTCHJnPb2",
        "outputId": "0a2e7b23-239a-466a-8df8-e0f4868f796f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Programming is something that I have a great passion for.', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Templates in Chat Models\n",
        "\n",
        "We have discussed templates in lesson 1 which helps us to create dynamic inputs. We can do the same with Chat Models as well. Let's discuss the code for that\n",
        "\n",
        "We define a system message with input variable task. This task can be dynamically change to do various tasks. For this example we will follow the task of paraphrasing"
      ],
      "metadata": {
        "id": "2WldCk-so6O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template=\"You are a helpful assistant that {task}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "chat(chat_prompt.format_prompt(task=\"paraphrases the sentence\", text=\"I love programming.\").to_messages())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1JWLOxwoIYE",
        "outputId": "d0748e8f-63a2-4124-a12f-f51cdcc00c9d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Programming is something that I have a great passion for.', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like how we used the LLMChain or SequentialChain for LLMs in lesson 1, we can do the same for Chat Models. Thus all the benefits we talked like chaining multiple tasks for LLMs can be achieved with Chat Models as well Here is an example"
      ],
      "metadata": {
        "id": "2b-ipRktpzc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "chain.run(task=\"paraphrases the sentence\", text=\"I love programming.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hBTlMb7XpZ_x",
        "outputId": "ee9a7415-5394-492d-da95-a8c379bc7f8b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Programming is something that I have a great passion for.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming with Chat Models\n",
        "\n",
        "We disucussed about how streaming is useful in the above section of LLMs. Now let's see how we can do the same with Chat Models"
      ],
      "metadata": {
        "id": "1gnDNh7tqRi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0, openai_api_key=\"openai-key\")\n",
        "resp = chat([HumanMessage(content=\"Write me a poem on beauty.\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BElza6zoqL5k",
        "outputId": "107d9bcb-a852-4e8b-83d0-02ee5d51e3e2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beauty is a thing of wonder,\n",
            "A sight that fills the heart with thunder,\n",
            "It's the sparkle in a lover's eye,\n",
            "Or the colors of a sunset sky.\n",
            "\n",
            "It's the way a flower blooms,\n",
            "Or the way a bird takes flight,\n",
            "It's the sound of a baby's coos,\n",
            "Or the stars that shine so bright.\n",
            "\n",
            "Beauty is in the laughter of a child,\n",
            "Or the way a mother's love is mild,\n",
            "It's the way a friend lends a hand,\n",
            "Or the way a stranger understands.\n",
            "\n",
            "It's the way the ocean waves crash,\n",
            "Or the way a mountain stands so tall,\n",
            "It's the way a painting can flash,\n",
            "Or the way a melody can enthrall.\n",
            "\n",
            "Beauty is all around us,\n",
            "In every moment, every place,\n",
            "It's up to us to see it thus,\n",
            "And let it fill our hearts with grace."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding models\n",
        "\n",
        "So far we have talked about text generation models but now we are going to talk about a completely different kind of model called Embedding models\n",
        "\n",
        "### Embeddings\n",
        "\n",
        "First we need to understand what is an embedding. An embedding is generally associated with a piece of text and it represents the properties of text\n",
        "\n",
        "Just to give an example, let's consider the words good, best, bad. If we find the embeddings of these words we observe that embeddings of good and best are close while embedding of bad is far. The reason being embedding of a word has knowledge of the meaning of the word. Thus words with similar meanings have similar embeddings\n",
        "\n",
        "Embeddings also have an interesting as can be seen below. Let's consider E(x) as Embedding of word x\n",
        "\n",
        "E(king) - E(male) + E(female) ~= E(queen)\n",
        "\n",
        "What this represents is if we subtract the embedding of word male from word king and add the embedding of word female it will be quite close to embedding of word queen. As humans we can understand this intuitively as removing male gender and adding female gender to king makes it a queen but now machines have the capability to understand such complex relations"
      ],
      "metadata": {
        "id": "I8Jm6gWhq5wG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use-cases\n",
        "\n",
        "Now that we have an idea of what is embeddings, the task of a embeddings model is to create these embeddings for the text input provided. A model which generates embedding which can show properties like the ones we discussed above and more is considered a good model.\n",
        "\n",
        "Once these embeddings are generated, we can use it to perform tasks like semantic search similar to how apps like Chatbase, PDF.ai, SiteGPT work. You can creating embeddings for all your documents or webpages and when user asks a query you can fetch the relevant pieces and send to the user\n",
        "\n",
        "Now let's discuss it with the help of an example"
      ],
      "metadata": {
        "id": "BIeDJtVKtLN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=\"openai-key\")\n",
        "text = \"This is dummy content.\"\n",
        "doc_result = embeddings.embed_documents([text])"
      ],
      "metadata": {
        "id": "BXNAymAqqiY3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the output is a vector which is a respresentation of text \"This is dummy content\". To identify if two sentences are similar, we can calculate the distance between these vectors. If the distance is small, then the words are of similar meanings"
      ],
      "metadata": {
        "id": "lLnrb5VJu3Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gKfYPHIuoLg",
        "outputId": "de4666a5-281b-47a4-c9d5-1b800003bb03"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.009644212425631677,\n",
              "  0.00568438101747335,\n",
              "  -0.02505903376829762,\n",
              "  -0.009684009702816118,\n",
              "  -0.011945822554121088,\n",
              "  0.014194370577687172,\n",
              "  -0.03528693772998923,\n",
              "  -0.025881510830109394,\n",
              "  -0.0183200244397753,\n",
              "  -0.038019687821944544,\n",
              "  0.004019527323934989,\n",
              "  0.0072829059157043085,\n",
              "  -0.015587276210465188,\n",
              "  0.009504921955486136,\n",
              "  -0.00734260183148097,\n",
              "  -0.006304555719258846,\n",
              "  0.017643468864994617,\n",
              "  -0.022007907321511987,\n",
              "  0.01074527135905713,\n",
              "  -0.017988380463238297,\n",
              "  -0.0033147834081325555,\n",
              "  0.0051736488279450635,\n",
              "  -0.009664111064223898,\n",
              "  0.017988380463238297,\n",
              "  -0.013318829548306883,\n",
              "  -0.007959460093501097,\n",
              "  0.01968639762310776,\n",
              "  -0.027247884013441855,\n",
              "  0.014141307541441252,\n",
              "  -0.00493154872507305,\n",
              "  0.03128067849342222,\n",
              "  -0.020734392588964694,\n",
              "  -0.005962961957764433,\n",
              "  -0.03711761620576389,\n",
              "  0.011481520986969281,\n",
              "  0.000909534129521696,\n",
              "  0.004513677314636424,\n",
              "  -0.03669311191579652,\n",
              "  0.016966915152859127,\n",
              "  0.0019119285724737535,\n",
              "  -0.020880315938640977,\n",
              "  0.007229842879458388,\n",
              "  0.0031953915765792337,\n",
              "  -0.04255658114626115,\n",
              "  -0.014884190048884142,\n",
              "  -0.009299302690033193,\n",
              "  -0.0013398424801167712,\n",
              "  -0.030723514750194858,\n",
              "  -0.0012395200607990026,\n",
              "  0.016874054839428766,\n",
              "  0.01578626259638739,\n",
              "  -0.006709161370633992,\n",
              "  -0.033429731461382006,\n",
              "  -0.010473322366974189,\n",
              "  -0.001961675285369629,\n",
              "  0.011866227999752206,\n",
              "  -0.013836195083027207,\n",
              "  0.020853784420518016,\n",
              "  0.01049985388509715,\n",
              "  -0.0003886455178349516,\n",
              "  -0.01821389836728346,\n",
              "  -0.005031041918034151,\n",
              "  -0.0070374893730669255,\n",
              "  -0.02216709643024975,\n",
              "  0.0009153378991110936,\n",
              "  -0.009491656196424655,\n",
              "  -0.004284842505164593,\n",
              "  -0.006390783153158467,\n",
              "  0.0016640245835970178,\n",
              "  0.00736250047007319,\n",
              "  0.018598605380066385,\n",
              "  0.00916664509941839,\n",
              "  -0.003730996831517146,\n",
              "  -0.02820302052851362,\n",
              "  0.02471412403269914,\n",
              "  0.0015836009192867942,\n",
              "  -0.02488657890049838,\n",
              "  0.010327399017297907,\n",
              "  0.0075813854945876126,\n",
              "  -0.011030485178878953,\n",
              "  0.022525271924909714,\n",
              "  -0.04616486761098376,\n",
              "  -0.025191691358912426,\n",
              "  0.018465947789451584,\n",
              "  0.03141333608403702,\n",
              "  0.01814756957197606,\n",
              "  0.02227322250274159,\n",
              "  0.030909237239700772,\n",
              "  -0.003624870759025304,\n",
              "  -0.012874426619747296,\n",
              "  -0.01581279411451035,\n",
              "  0.017325090647519092,\n",
              "  0.017444482479072414,\n",
              "  0.013584144729536486,\n",
              "  -0.011448356589315579,\n",
              "  0.035764508781492906,\n",
              "  0.001513126457857356,\n",
              "  0.019341487887509275,\n",
              "  -0.014950518844191543,\n",
              "  -0.047120002263410336,\n",
              "  -0.01235706201634957,\n",
              "  0.00044025764078735406,\n",
              "  -0.01476479821733082,\n",
              "  0.0032484546128251543,\n",
              "  -0.029954100724629007,\n",
              "  -0.016728131489752485,\n",
              "  -0.000562551444446868,\n",
              "  -0.0025719004350283664,\n",
              "  0.028574461782235067,\n",
              "  0.013690270802028328,\n",
              "  -0.004543525272524755,\n",
              "  0.0043345891016451435,\n",
              "  -0.004629752706424376,\n",
              "  -0.04215860464912636,\n",
              "  -0.004725929459620107,\n",
              "  0.01578626259638739,\n",
              "  -0.011083548215124875,\n",
              "  -0.006261442002309036,\n",
              "  0.003691199554332705,\n",
              "  -0.012118277421920329,\n",
              "  0.0026912924994123375,\n",
              "  0.02361306416795109,\n",
              "  0.015971983223248113,\n",
              "  -0.023533469613582207,\n",
              "  0.0075813854945876126,\n",
              "  0.0071568812046202464,\n",
              "  -0.0062349104841860755,\n",
              "  -0.022512006165848232,\n",
              "  0.01306014724660802,\n",
              "  -0.004364437059533474,\n",
              "  0.018014911981361258,\n",
              "  0.016661802694445085,\n",
              "  0.0072829059157043085,\n",
              "  0.026080497216031596,\n",
              "  -0.01627709568166216,\n",
              "  0.034623649776915226,\n",
              "  -0.01955374003249296,\n",
              "  0.007760473707578893,\n",
              "  -0.0248733131414369,\n",
              "  0.006301239279493476,\n",
              "  -0.003158910739160163,\n",
              "  0.027354010085933696,\n",
              "  -0.025258020154219827,\n",
              "  -0.008695709721413248,\n",
              "  0.007654347169425754,\n",
              "  0.015534213174219267,\n",
              "  -0.008781937155312869,\n",
              "  -0.012549415522741032,\n",
              "  0.000390096460232301,\n",
              "  -0.023188559877983725,\n",
              "  -0.010844764552018231,\n",
              "  0.016011780500432553,\n",
              "  0.021689529104036463,\n",
              "  -0.012297366100572908,\n",
              "  0.007508423819749472,\n",
              "  0.03212968707335695,\n",
              "  0.012244303064326988,\n",
              "  -0.00911358206317247,\n",
              "  -0.002314876353212188,\n",
              "  -0.005478761752020406,\n",
              "  -0.007269640156642829,\n",
              "  0.009239606774256531,\n",
              "  0.0006748958997064407,\n",
              "  -0.013637207765782408,\n",
              "  0.024966173454867263,\n",
              "  0.022485474647725275,\n",
              "  0.02549680381732647,\n",
              "  0.002528786718078556,\n",
              "  -0.016754663007875446,\n",
              "  -0.022724258310831916,\n",
              "  -0.02348040657733629,\n",
              "  0.020283356780874368,\n",
              "  -0.018903717838480428,\n",
              "  0.023812050553873292,\n",
              "  -0.029290812771554996,\n",
              "  0.007448727903972811,\n",
              "  0.005870101644334072,\n",
              "  0.007309437433827269,\n",
              "  -0.016661802694445085,\n",
              "  -0.011660608734299263,\n",
              "  -0.032580722881447276,\n",
              "  0.0001352486319410751,\n",
              "  0.0058502030057418514,\n",
              "  0.02937040732592388,\n",
              "  -0.009067151906457288,\n",
              "  0.010247804462929026,\n",
              "  0.012920856776462477,\n",
              "  -0.004765726736804548,\n",
              "  -0.011939189674590346,\n",
              "  -0.0008805152815747081,\n",
              "  0.029264281253432035,\n",
              "  0.028282615082882503,\n",
              "  -0.00655992158119234,\n",
              "  -0.005243294063017835,\n",
              "  -0.6910934012993952,\n",
              "  -0.002527128498195871,\n",
              "  0.009856464570615361,\n",
              "  -0.01623729840447772,\n",
              "  0.00823804103379218,\n",
              "  0.02743360464030258,\n",
              "  0.007879865539132216,\n",
              "  0.0024160277660559746,\n",
              "  -0.021331353609376498,\n",
              "  0.011813164963506284,\n",
              "  -0.010911093347325632,\n",
              "  -0.009491656196424655,\n",
              "  0.013637207765782408,\n",
              "  -0.005435648035070595,\n",
              "  0.01149478674603076,\n",
              "  -0.015918920187002195,\n",
              "  0.0017842456415070066,\n",
              "  -0.006079037349552385,\n",
              "  -0.018545542343820463,\n",
              "  0.031890903410250304,\n",
              "  0.005193547466537284,\n",
              "  0.010181475667621625,\n",
              "  -0.020416014371489172,\n",
              "  0.007515056699280212,\n",
              "  -0.009723806980000558,\n",
              "  0.003145644980098683,\n",
              "  -0.0014542596520220378,\n",
              "  -0.00490501720695009,\n",
              "  0.009286036930971712,\n",
              "  0.027619327129808493,\n",
              "  -0.016462816308522882,\n",
              "  0.009027354629272849,\n",
              "  -0.006609668177672891,\n",
              "  0.003143986760215998,\n",
              "  0.04123000151482275,\n",
              "  0.012808097824439896,\n",
              "  -0.010911093347325632,\n",
              "  -0.005485394631551147,\n",
              "  0.0014418230029019001,\n",
              "  0.010605980888911588,\n",
              "  -0.03032554197835045,\n",
              "  0.003551909084187163,\n",
              "  0.024196757566656218,\n",
              "  -0.00890796186639693,\n",
              "  -0.027128492181888533,\n",
              "  -0.01309994452379246,\n",
              "  0.0009136796792284086,\n",
              "  0.006672680533214921,\n",
              "  0.0048818021285925,\n",
              "  0.015627073487649627,\n",
              "  -0.0014700127409075455,\n",
              "  -0.003137353880685258,\n",
              "  -0.0020876999964536905,\n",
              "  -0.0037243639519864056,\n",
              "  -0.01541482041134335,\n",
              "  0.004032793082996469,\n",
              "  0.02102623928831726,\n",
              "  0.012071847265205148,\n",
              "  0.016184235368231797,\n",
              "  0.012788199185847676,\n",
              "  -0.010188108547152365,\n",
              "  0.005826987927384261,\n",
              "  -0.01808124077666866,\n",
              "  -0.0036414529578521542,\n",
              "  -0.0014260699140163924,\n",
              "  -0.020110901913075126,\n",
              "  -0.0021656363309398865,\n",
              "  0.018200632608221978,\n",
              "  0.012788199185847676,\n",
              "  -0.022817118624262278,\n",
              "  -0.0013431589198821413,\n",
              "  0.035340000766235155,\n",
              "  -0.006891565557729345,\n",
              "  -0.002367939389458109,\n",
              "  -0.009518187714547615,\n",
              "  0.020681329552718775,\n",
              "  0.030431668050842294,\n",
              "  -0.006085670229083125,\n",
              "  -0.022074236116819388,\n",
              "  0.019845586731845524,\n",
              "  0.009975856402168682,\n",
              "  -0.006937996180105824,\n",
              "  -0.013902523878334608,\n",
              "  -0.010101881113252744,\n",
              "  0.020734392588964694,\n",
              "  -0.0183200244397753,\n",
              "  -0.033695046642611616,\n",
              "  -0.0024326099648828248,\n",
              "  0.023201825637045204,\n",
              "  0.0031108223625622975,\n",
              "  0.02576211899855607,\n",
              "  0.002309901693564133,\n",
              "  -0.011653975854768522,\n",
              "  -0.016701599971629524,\n",
              "  0.012947388294585438,\n",
              "  0.021238493295946136,\n",
              "  0.0074818923016265115,\n",
              "  0.016993446670982088,\n",
              "  0.017205698815965773,\n",
              "  -0.011262635962454857,\n",
              "  -0.01077180287718009,\n",
              "  0.010400360692136049,\n",
              "  -0.015494415897034828,\n",
              "  0.009697275461877599,\n",
              "  0.034544055222546347,\n",
              "  0.016927117875674687,\n",
              "  -0.0012975577566929784,\n",
              "  0.027725453202300333,\n",
              "  0.018545542343820463,\n",
              "  -0.02464779523739174,\n",
              "  0.004709347260793257,\n",
              "  -0.012556048402271772,\n",
              "  0.004971346002257491,\n",
              "  0.011660608734299263,\n",
              "  -0.02227322250274159,\n",
              "  -0.03581757181773883,\n",
              "  0.012648908715702133,\n",
              "  -0.0020064472222021245,\n",
              "  0.010393727812605308,\n",
              "  -0.0019434348666600935,\n",
              "  0.028680587854726908,\n",
              "  0.024024302698856977,\n",
              "  0.02363959568607405,\n",
              "  0.007660980048956494,\n",
              "  -0.0182536956444679,\n",
              "  0.020548671962103974,\n",
              "  0.021410948163745377,\n",
              "  0.002522153838547816,\n",
              "  -0.02883977696346467,\n",
              "  -0.0037940091870591764,\n",
              "  -0.0044771964772173534,\n",
              "  -0.0030892655040873924,\n",
              "  0.03128067849342222,\n",
              "  -0.012894325258339516,\n",
              "  0.025828447793863476,\n",
              "  0.01472500094014638,\n",
              "  -0.007302804554296529,\n",
              "  0.0178557210099783,\n",
              "  0.03170518278338959,\n",
              "  -0.0014202661444269948,\n",
              "  -0.019142501501587073,\n",
              "  0.00042616274268070025,\n",
              "  -0.003774110548466956,\n",
              "  -0.013557613211413527,\n",
              "  -0.0015222467836274484,\n",
              "  -0.03711761620576389,\n",
              "  -0.026385609674445643,\n",
              "  0.011634077216176302,\n",
              "  0.02220689370743419,\n",
              "  0.0022170411473031223,\n",
              "  -0.021848718212774225,\n",
              "  0.014844392771699702,\n",
              "  -0.007939561454908876,\n",
              "  0.010725372720464909,\n",
              "  -0.020694595311780254,\n",
              "  -0.004032793082996469,\n",
              "  -0.0010537993175229554,\n",
              "  -0.020615000757411375,\n",
              "  0.0018920299338815332,\n",
              "  0.00916001221988765,\n",
              "  0.006831869641952684,\n",
              "  0.019421082441878155,\n",
              "  -0.009252872533318012,\n",
              "  0.010314133258236427,\n",
              "  -0.023891645108242175,\n",
              "  0.013418322741267984,\n",
              "  -0.008980924472557668,\n",
              "  0.0012420073906230303,\n",
              "  0.0009791794228021294,\n",
              "  -0.037382931386993494,\n",
              "  -0.007269640156642829,\n",
              "  0.0034490992186300426,\n",
              "  0.00284716516838473,\n",
              "  0.00035320103873872803,\n",
              "  -0.006420631111046798,\n",
              "  0.009518187714547615,\n",
              "  -0.027327478567810735,\n",
              "  -0.0027012418187084474,\n",
              "  0.002664760981289377,\n",
              "  -0.005996126355418133,\n",
              "  -0.0075813854945876126,\n",
              "  0.019076172706279673,\n",
              "  -0.00890132898686619,\n",
              "  0.0010695524064084632,\n",
              "  0.014260699372994573,\n",
              "  0.009756971377654258,\n",
              "  -0.002749330195306313,\n",
              "  0.019978244322460325,\n",
              "  -0.01049985388509715,\n",
              "  0.0013066779660477462,\n",
              "  0.009100316304110989,\n",
              "  0.011693773131952964,\n",
              "  0.0022933194947372827,\n",
              "  0.011925923915528866,\n",
              "  -0.0183200244397753,\n",
              "  0.0033960361823841215,\n",
              "  -0.03510121710312852,\n",
              "  0.01968639762310776,\n",
              "  0.005939746879406843,\n",
              "  0.004878485688827129,\n",
              "  0.02107930232456318,\n",
              "  0.0055683056256853975,\n",
              "  0.023533469613582207,\n",
              "  -0.02053540620304249,\n",
              "  -0.0019550424058388887,\n",
              "  -0.036799237988288366,\n",
              "  -0.010075349595129783,\n",
              "  -0.0004721783735290408,\n",
              "  0.01577299683732591,\n",
              "  0.004258310987041632,\n",
              "  -0.0005695988789482793,\n",
              "  -0.015826059873571833,\n",
              "  0.018744528729742665,\n",
              "  0.0040725903601809095,\n",
              "  -0.0039366163298007375,\n",
              "  -0.0043246397823490336,\n",
              "  -0.007415563506319111,\n",
              "  0.011700406011483703,\n",
              "  -0.004613170507597525,\n",
              "  -0.00406927392041554,\n",
              "  0.01577299683732591,\n",
              "  -0.0343052715594397,\n",
              "  -0.003064392205847117,\n",
              "  -0.006977793457290264,\n",
              "  0.004692765061966407,\n",
              "  -0.00015307451023185468,\n",
              "  0.01574646531920295,\n",
              "  0.025894776589170877,\n",
              "  0.02143747968186834,\n",
              "  -0.03669311191579652,\n",
              "  0.0003869872979522665,\n",
              "  0.022963041973938562,\n",
              "  -0.0016540752643009076,\n",
              "  -0.00740229774725763,\n",
              "  -0.0025321031578439263,\n",
              "  -0.005375952119293935,\n",
              "  0.010420259330728269,\n",
              "  -0.00575070981278075,\n",
              "  0.024541669164899894,\n",
              "  -0.010148311269967925,\n",
              "  -0.008125282081769599,\n",
              "  0.020668063793657293,\n",
              "  0.036799237988288366,\n",
              "  0.016529145103830283,\n",
              "  0.033721578160734574,\n",
              "  0.007966092973031837,\n",
              "  0.01415457330050273,\n",
              "  0.006689262732041772,\n",
              "  0.01081823303389527,\n",
              "  0.027407073122179618,\n",
              "  -0.013531081693290566,\n",
              "  0.01046668948744345,\n",
              "  4.757021198835776e-05,\n",
              "  0.00028189755467944086,\n",
              "  0.01895678087472635,\n",
              "  -0.030590857159580057,\n",
              "  -0.009557984991732056,\n",
              "  -0.009365631485340593,\n",
              "  0.024554934923961377,\n",
              "  0.016025046259494035,\n",
              "  0.022963041973938562,\n",
              "  -0.016330158717908078,\n",
              "  0.014552546072347136,\n",
              "  -0.0147780639763923,\n",
              "  0.03483590192189891,\n",
              "  0.003198708016344604,\n",
              "  -0.02475392130988358,\n",
              "  0.006937996180105824,\n",
              "  0.01150805250509224,\n",
              "  -0.020044573117767726,\n",
              "  -0.02210076763494235,\n",
              "  0.004573373230413085,\n",
              "  0.01051311964415863,\n",
              "  -0.018041443499484215,\n",
              "  0.004228463029153302,\n",
              "  0.0076079170127105735,\n",
              "  0.012018784228959228,\n",
              "  0.016966915152859127,\n",
              "  0.011090181094655614,\n",
              "  0.021941578526204587,\n",
              "  -0.027513199194671458,\n",
              "  -0.03711761620576389,\n",
              "  -0.004526943073697904,\n",
              "  0.01243002369118771,\n",
              "  0.0011458306374272987,\n",
              "  -0.002248547557904787,\n",
              "  -0.03205009251898807,\n",
              "  -0.006301239279493476,\n",
              "  -0.030909237239700772,\n",
              "  0.0011018876941208211,\n",
              "  0.001536341652630271,\n",
              "  0.014234167854871612,\n",
              "  0.014313762409240493,\n",
              "  -0.015971983223248113,\n",
              "  -0.0010297551292240226,\n",
              "  0.021318087850315016,\n",
              "  0.003415934820976342,\n",
              "  0.007349234711011709,\n",
              "  -0.023453875059213328,\n",
              "  -0.00576397557184223,\n",
              "  0.01074527135905713,\n",
              "  0.005691013897004089,\n",
              "  0.018094506535730137,\n",
              "  -0.00573081117418853,\n",
              "  0.009186543738010611,\n",
              "  0.016794460285059886,\n",
              "  -0.00733596895195023,\n",
              "  0.00734260183148097,\n",
              "  0.00244421750406162,\n",
              "  0.01211164454238959,\n",
              "  0.00037848889194967467,\n",
              "  0.016038312018555514,\n",
              "  -0.00818497799754626,\n",
              "  -4.684473715170417e-05,\n",
              "  0.012224404425734766,\n",
              "  -0.0036547187169136343,\n",
              "  -0.0008680786324545705,\n",
              "  0.007103818168374326,\n",
              "  0.014950518844191543,\n",
              "  0.005256559822079315,\n",
              "  -0.012290733221042169,\n",
              "  -0.008443660299245124,\n",
              "  -0.021119101464392814,\n",
              "  0.015971983223248113,\n",
              "  0.05492027231685107,\n",
              "  0.017285293370334652,\n",
              "  0.003180467597635069,\n",
              "  0.010181475667621625,\n",
              "  0.008781937155312869,\n",
              "  -0.014923987326068583,\n",
              "  -0.027486667676548497,\n",
              "  0.004815473333285098,\n",
              "  0.0025536600163188314,\n",
              "  -0.007461993663034292,\n",
              "  -0.01577299683732591,\n",
              "  0.012781566306316935,\n",
              "  0.01077843575671083,\n",
              "  -0.012861160860685816,\n",
              "  0.021769123658405342,\n",
              "  -0.013371892584552804,\n",
              "  -0.003347947805786256,\n",
              "  -0.008085484804585158,\n",
              "  -0.00818497799754626,\n",
              "  -0.018691465693496747,\n",
              "  -0.0025719004350283664,\n",
              "  0.006656098334388071,\n",
              "  -0.0007884840198780271,\n",
              "  -0.0010446791081681877,\n",
              "  0.021105835705331335,\n",
              "  -0.005614735782400578,\n",
              "  0.018731262970681187,\n",
              "  0.01051975252368937,\n",
              "  0.010990687901694513,\n",
              "  -0.01955374003249296,\n",
              "  -0.01247645384790289,\n",
              "  0.008012523129747018,\n",
              "  0.011083548215124875,\n",
              "  0.02277732134707784,\n",
              "  -0.018797591765988587,\n",
              "  0.0196598661049848,\n",
              "  0.002323167452625613,\n",
              "  -0.0082579396723844,\n",
              "  0.016688334212568046,\n",
              "  -0.008569685010329186,\n",
              "  0.0021689527707052566,\n",
              "  0.01963333458686184,\n",
              "  9.48813198440882e-05,\n",
              "  -0.013411689861737245,\n",
              "  0.02952959643466164,\n",
              "  -0.022114033394003828,\n",
              "  -0.003502162487706612,\n",
              "  0.026425406951630082,\n",
              "  -0.005157066629118213,\n",
              "  -0.0197925236955996,\n",
              "  0.010327399017297907,\n",
              "  -0.010977422142633033,\n",
              "  -0.012914223896931738,\n",
              "  -0.013491284416106126,\n",
              "  0.021172164500638736,\n",
              "  0.010479955246504928,\n",
              "  -0.005226711864190985,\n",
              "  0.009279404051440973,\n",
              "  -0.016011780500432553,\n",
              "  -0.02937040732592388,\n",
              "  -0.01621076688635476,\n",
              "  -0.005196863906302654,\n",
              "  -0.009312568449094673,\n",
              "  0.0048818021285925,\n",
              "  -0.026544798783183405,\n",
              "  -0.037807435676960856,\n",
              "  0.0026365710104530827,\n",
              "  -0.004759093857273807,\n",
              "  -0.019155767260648552,\n",
              "  0.010632512407034549,\n",
              "  0.0020230294210289747,\n",
              "  -0.0038006420665899166,\n",
              "  0.0063443529964432865,\n",
              "  0.0008937810406361884,\n",
              "  0.01243665657071845,\n",
              "  -0.004689448622201037,\n",
              "  -0.003873603974258707,\n",
              "  -0.01079170151577231,\n",
              "  0.0026398876830491017,\n",
              "  0.014406622722670855,\n",
              "  0.0001868607548934776,\n",
              "  -0.053699822483194895,\n",
              "  -0.008191610877077,\n",
              "  -0.02560292988981831,\n",
              "  -0.0046032211883014155,\n",
              "  0.015282162820728548,\n",
              "  -0.004294791824460703,\n",
              "  0.005170332388179694,\n",
              "  -0.0069114646619828635,\n",
              "  0.021915047008081626,\n",
              "  -0.0026398876830491017,\n",
              "  0.008947760074903967,\n",
              "  0.0053792685590593045,\n",
              "  -0.015799528355448872,\n",
              "  0.022896713178631158,\n",
              "  0.014247433613933092,\n",
              "  0.021265024814069097,\n",
              "  -0.012861160860685816,\n",
              "  0.01046668948744345,\n",
              "  0.008470191817368083,\n",
              "  -0.011096813974186355,\n",
              "  -0.010592715129850107,\n",
              "  0.00031423284239179885,\n",
              "  0.005535141228031697,\n",
              "  0.01642301903133844,\n",
              "  -0.016860789080367287,\n",
              "  0.0006446333286397767,\n",
              "  -0.0009410403072927115,\n",
              "  0.0006081524912207062,\n",
              "  -0.009790135775307959,\n",
              "  -0.014698469422023418,\n",
              "  -0.00736250047007319,\n",
              "  -0.008762038516720649,\n",
              "  -0.008516621974083264,\n",
              "  0.00490833364671546,\n",
              "  -0.005157066629118213,\n",
              "  0.0056512166198196484,\n",
              "  0.01969966338216924,\n",
              "  -0.014300496650179014,\n",
              "  -0.016542410862891762,\n",
              "  -0.003472314529818282,\n",
              "  -0.0331644162801524,\n",
              "  0.013756599597335729,\n",
              "  0.010924359106387112,\n",
              "  0.002809026111082974,\n",
              "  0.012954021174116178,\n",
              "  -0.019381285164693715,\n",
              "  -0.019182298778771513,\n",
              "  -0.01406171298707237,\n",
              "  0.02292324469675412,\n",
              "  -0.025921308107293834,\n",
              "  0.0010670650765844357,\n",
              "  -0.012914223896931738,\n",
              "  -0.01902310967003375,\n",
              "  -0.014963784603253023,\n",
              "  0.009717174100469819,\n",
              "  -0.003920034130973887,\n",
              "  0.010473322366974189,\n",
              "  -0.007700777326140934,\n",
              "  -0.015507681656096306,\n",
              "  -0.0033363402666074606,\n",
              "  0.009412061642055774,\n",
              "  0.012675440233825094,\n",
              "  -0.021477276959052778,\n",
              "  -0.005959645517999063,\n",
              "  -0.030935768757823733,\n",
              "  -0.012814730703970635,\n",
              "  -0.008078851925054418,\n",
              "  -0.009365631485340593,\n",
              "  0.006785439485237504,\n",
              "  0.026385609674445643,\n",
              "  0.008815101552966569,\n",
              "  -0.00984319881155388,\n",
              "  -0.02830914660100546,\n",
              "  0.006112202212867383,\n",
              "  -0.02225995674368011,\n",
              "  -0.0068782997986678645,\n",
              "  0.0029632405601726815,\n",
              "  0.03382770423322642,\n",
              "  0.02562946140794127,\n",
              "  0.03990342560867473,\n",
              "  -0.011315698998700777,\n",
              "  0.026558064542244884,\n",
              "  0.022498740406786753,\n",
              "  -0.0011508052970753539,\n",
              "  -0.0011856279146117393,\n",
              "  0.01627709568166216,\n",
              "  -0.0003453244942959742,\n",
              "  -0.029980632242751968,\n",
              "  0.006755591527349173,\n",
              "  0.003198708016344604,\n",
              "  0.02149054271811426,\n",
              "  0.011083548215124875,\n",
              "  0.01537502313415891,\n",
              "  -0.0044771964772173534,\n",
              "  0.007216577120396908,\n",
              "  0.0019732828245484237,\n",
              "  -0.004394285017421804,\n",
              "  -0.017577140069687216,\n",
              "  -0.022087501875880867,\n",
              "  -0.027725453202300333,\n",
              "  0.0013945637362453769,\n",
              "  -0.008947760074903967,\n",
              "  0.02952959643466164,\n",
              "  -0.023904910867303654,\n",
              "  -0.01570666804201851,\n",
              "  -0.010254437342459765,\n",
              "  0.034490992186300425,\n",
              "  0.012715237511009534,\n",
              "  -0.005956329078233693,\n",
              "  0.015865857150756273,\n",
              "  -0.016701599971629524,\n",
              "  0.004404234336717915,\n",
              "  -0.0035087953672373524,\n",
              "  -0.00922634101519505,\n",
              "  -0.02228648826180307,\n",
              "  0.01045342372838197,\n",
              "  -0.006221644725124595,\n",
              "  -0.0033728211040265316,\n",
              "  0.007488525181157252,\n",
              "  -0.0011731912654916017,\n",
              "  -0.0071568812046202464,\n",
              "  0.0015811135894627667,\n",
              "  0.007117083927435806,\n",
              "  -0.004822106212815838,\n",
              "  0.0015471200818677237,\n",
              "  -0.020734392588964694,\n",
              "  -0.011906025276936646,\n",
              "  0.0009153378991110936,\n",
              "  -0.01053965116228159,\n",
              "  -0.014472951517978255,\n",
              "  -0.02696930307315077,\n",
              "  -0.014711735181084899,\n",
              "  -0.027698921684177372,\n",
              "  0.0020959910958671156,\n",
              "  0.018651668416312304,\n",
              "  -0.015574010451403709,\n",
              "  0.009266138292379492,\n",
              "  0.002596773733268642,\n",
              "  -0.027048897627519653,\n",
              "  -0.010234538703867545,\n",
              "  -0.009989122161230162,\n",
              "  0.053487570338211214,\n",
              "  0.015308694338851509,\n",
              "  0.019248627574078914,\n",
              "  0.022604866479278594,\n",
              "  -0.020721126829903215,\n",
              "  -0.02219362794837271,\n",
              "  0.0033048340888364456,\n",
              "  -0.006685946292276402,\n",
              "  -0.004314690463052923,\n",
              "  0.023162028359860764,\n",
              "  0.020561937721165453,\n",
              "  -0.018837389043173027,\n",
              "  0.005783874210434451,\n",
              "  -0.016011780500432553,\n",
              "  0.01774959493748646,\n",
              "  0.016131172331985875,\n",
              "  -0.023069168046430402,\n",
              "  -0.011348863396354478,\n",
              "  0.014658672144838979,\n",
              "  0.0016474423847701676,\n",
              "  -0.004022843763700359,\n",
              "  -0.011017219419817474,\n",
              "  -0.033986893341964176,\n",
              "  0.009889628968269061,\n",
              "  -0.02097317625207134,\n",
              "  -0.01700671243004357,\n",
              "  -0.021424213922806856,\n",
              "  -0.0005555039517377944,\n",
              "  -0.032368470736463595,\n",
              "  0.003608288560198454,\n",
              "  -0.016568942381014723,\n",
              "  0.024634529478330256,\n",
              "  -0.003989679366046659,\n",
              "  -0.021158898741577253,\n",
              "  -0.025364146226711667,\n",
              "  -0.029184686699063156,\n",
              "  -0.023931442385426615,\n",
              "  0.001633347515767345,\n",
              "  0.006669364093449552,\n",
              "  0.018704731452558226,\n",
              "  0.0037873763075284366,\n",
              "  -0.008065586165992938,\n",
              "  0.015295428579790028,\n",
              "  0.003069366865495172,\n",
              "  -0.002936709042049721,\n",
              "  0.008509989094552525,\n",
              "  -0.009949324884045721,\n",
              "  0.0248733131414369,\n",
              "  -0.004643018465485856,\n",
              "  -0.005860152325037961,\n",
              "  0.008284471190507361,\n",
              "  -0.008622748046575106,\n",
              "  0.015441352860788906,\n",
              "  -0.008609482287513626,\n",
              "  -0.01049985388509715,\n",
              "  -0.0028455069485020445,\n",
              "  -0.028043831419775858,\n",
              "  -0.006009392114479614,\n",
              "  0.002774203493546589,\n",
              "  -0.004251678107510893,\n",
              "  -0.013484651536575385,\n",
              "  -0.008145180720361819,\n",
              "  -0.012297366100572908,\n",
              "  -0.011169775649024495,\n",
              "  -0.020707861070841736,\n",
              "  0.002394470907581069,\n",
              "  -0.013279032271122442,\n",
              "  -0.039505452836830324,\n",
              "  -0.0183200244397753,\n",
              "  -0.011302433239639297,\n",
              "  0.00027153365130882834,\n",
              "  0.010062083836068303,\n",
              "  -0.0036348200783214144,\n",
              "  -0.00736250047007319,\n",
              "  0.019938447045275885,\n",
              "  0.02679684820535153,\n",
              "  -0.027698921684177372,\n",
              "  0.0039366163298007375,\n",
              "  -0.014234167854871612,\n",
              "  0.007694144446610195,\n",
              "  -0.0049381816046037905,\n",
              "  0.00974370561859278,\n",
              "  0.005943063319172213,\n",
              "  -0.008801835793905088,\n",
              "  -0.0026846596198815973,\n",
              "  -0.010446790848851228,\n",
              "  -0.015653605005772588,\n",
              "  -0.03770130960446902,\n",
              "  0.007322703192888749,\n",
              "  0.005419065836243745,\n",
              "  -0.023838582071996253,\n",
              "  -0.010280968860582726,\n",
              "  0.02690297427784337,\n",
              "  0.003893502612850927,\n",
              "  0.007939561454908876,\n",
              "  0.015653605005772588,\n",
              "  -0.015030113398560424,\n",
              "  0.016847523321305808,\n",
              "  0.0016930434315440058,\n",
              "  -0.019288424851263353,\n",
              "  0.007561486855995393,\n",
              "  -0.030458199568965255,\n",
              "  -0.0011143243432409587,\n",
              "  0.004812156893519728,\n",
              "  -0.0012362036210336327,\n",
              "  0.020455811648673612,\n",
              "  -0.023241622914229643,\n",
              "  -0.020031307358706243,\n",
              "  -0.009551352112201317,\n",
              "  0.016542410862891762,\n",
              "  0.014910721567007103,\n",
              "  -0.011554482661807421,\n",
              "  -0.027062163386581132,\n",
              "  -0.014977050362314503,\n",
              "  -0.015003581880437464,\n",
              "  0.010274335981051987,\n",
              "  0.0030229364759493424,\n",
              "  0.006805338123829724,\n",
              "  0.004609854067832156,\n",
              "  0.004420816535544765,\n",
              "  -0.02284365014238524,\n",
              "  0.017364887924703535,\n",
              "  -0.0010471664379922154,\n",
              "  0.0017212331695496512,\n",
              "  -0.014128041782379771,\n",
              "  -0.021371150886560938,\n",
              "  -0.027937705347284018,\n",
              "  -0.004162134233845901,\n",
              "  0.003595022801136974,\n",
              "  0.01835982171695974,\n",
              "  0.021941578526204587,\n",
              "  -0.02499270497299022,\n",
              "  -0.012257568823388468,\n",
              "  -0.002195484288828217,\n",
              "  -0.030909237239700772,\n",
              "  -0.01640975327227696,\n",
              "  -0.0182536956444679,\n",
              "  -0.010579449370788627,\n",
              "  0.01849247930757454,\n",
              "  -0.005999442795183504,\n",
              "  -0.0019898650233752743,\n",
              "  0.03608288699896843,\n",
              "  0.012038682867551448,\n",
              "  0.0215966687906061,\n",
              "  0.0012071847730866446,\n",
              "  -0.00410243831806924,\n",
              "  0.012071847265205148,\n",
              "  0.0017328408251437708,\n",
              "  0.004613170507597525,\n",
              "  0.011123345492309315,\n",
              "  -0.00406927392041554,\n",
              "  -0.020681329552718775,\n",
              "  0.03560531594746476,\n",
              "  0.02154360575436018,\n",
              "  0.0081584464794233,\n",
              "  0.013577511850005747,\n",
              "  0.00203463696020777,\n",
              "  -0.01147488810743854,\n",
              "  -0.01051975252368937,\n",
              "  0.0009219707786418338,\n",
              "  -0.007826802502886294,\n",
              "  0.009889628968269061,\n",
              "  0.019500676996247038,\n",
              "  -0.014406622722670855,\n",
              "  -0.010088615354191264,\n",
              "  0.0010007362812770345,\n",
              "  0.01782918949185534,\n",
              "  -0.002671393860820117,\n",
              "  0.007879865539132216,\n",
              "  -0.002669735640937432,\n",
              "  0.009047253267865069,\n",
              "  -0.0017195749496669661,\n",
              "  -0.03019288438773565,\n",
              "  -0.0028537980479154696,\n",
              "  -0.028574461782235067,\n",
              "  -0.020747658348026176,\n",
              "  0.003134037440919888,\n",
              "  0.003611604999963824,\n",
              "  -0.030511262605211174,\n",
              "  0.007667612928487234,\n",
              "  0.018067975017607176,\n",
              "  -0.01648934782664584,\n",
              "  -0.008589583648921406,\n",
              "  0.015401554652281869,\n",
              "  -0.02044254588961213,\n",
              "  -0.020628266516472853,\n",
              "  -0.001264393359039278,\n",
              "  -0.02218036218931123,\n",
              "  0.005452230233897446,\n",
              "  -0.00651349142447716,\n",
              "  0.010997320781225253,\n",
              "  -0.015030113398560424,\n",
              "  -0.0019500677461908335,\n",
              "  -0.018744528729742665,\n",
              "  -0.0028372158490886194,\n",
              "  -0.00407590679994628,\n",
              "  0.013365259705022064,\n",
              "  -0.006871666919137124,\n",
              "  -0.015295428579790028,\n",
              "  -0.012350429136818828,\n",
              "  -0.011985619831305527,\n",
              "  0.007946194334439617,\n",
              "  0.005634634420992798,\n",
              "  -0.005263192701610055,\n",
              "  -0.00575402625254612,\n",
              "  -0.024568200683022855,\n",
              "  -0.004039425962527209,\n",
              "  0.012469820968372151,\n",
              "  -0.03502162254875963,\n",
              "  -0.04576689483913936,\n",
              "  0.014884190048884142,\n",
              "  0.012522884004618071,\n",
              "  -0.021517074236237218,\n",
              "  0.029635722507153482,\n",
              "  0.21639118102015723,\n",
              "  -0.0001705894722946308,\n",
              "  0.006493592785884939,\n",
              "  0.024541669164899894,\n",
              "  -4.1559161275645656e-05,\n",
              "  0.025961105384478277,\n",
              "  -0.01079170151577231,\n",
              "  -0.006334403677147177,\n",
              "  -0.013272399391591702,\n",
              "  0.00739566486772689,\n",
              "  0.01252951688414881,\n",
              "  0.01768326614217906,\n",
              "  -0.01316627331909986,\n",
              "  -0.0024574832631231,\n",
              "  -0.007190045602273948,\n",
              "  -0.009644212425631677,\n",
              "  -0.015295428579790028,\n",
              "  -0.00911358206317247,\n",
              "  -0.04629752520159856,\n",
              "  -0.025098831045482065,\n",
              "  -0.001875447735054683,\n",
              "  0.005455546673662816,\n",
              "  0.016874054839428766,\n",
              "  -0.00572749473442316,\n",
              "  0.01311984316238468,\n",
              "  -0.021861983971835704,\n",
              "  -0.028017299901652897,\n",
              "  0.0017676634426801565,\n",
              "  0.03345626297950497,\n",
              "  0.034411397631931545,\n",
              "  -0.010161577029029405,\n",
              "  -0.0006566554227892431,\n",
              "  0.003402669061914862,\n",
              "  0.012343796257288089,\n",
              "  -0.012502985366025851,\n",
              "  0.01623729840447772,\n",
              "  0.00824467391332292,\n",
              "  -0.0014907404894411082,\n",
              "  0.04146878517792939,\n",
              "  0.008569685010329186,\n",
              "  0.007966092973031837,\n",
              "  -0.02030988829899733,\n",
              "  0.011149877010432276,\n",
              "  -0.007263007277112089,\n",
              "  -0.008383964383468463,\n",
              "  0.01240349217306475,\n",
              "  ...]]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8amwnhjNuvfN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}